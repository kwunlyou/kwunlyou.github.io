---
layout: default
title: Kun Liu's Research
permalink: /research/surface_reconstruction_for_multi_view_point_data
home: passive
resume: passsive
research: active
blog: passive
contact: passive
---
<div>
    <h2 class="paper-title">Sphere Packing Aided Surface Reconstruction for Multi-View Data</h2>
    <h3 class="paper-authors">
        Kun Liu<sup>1,2</sup>
        <a style="padding-left:68px;color: black"
           href="http://www.alejandrogalindo.com/" target="_blank">Patricio A. Galindo<sup>1</sup>
        </a>
        <a style="padding-left:68px;color: black"
           href="http://people.mpi-inf.mpg.de/~rzayer/" target="_blank">Rhaleb Zayer<sup>1</sup>
        </a>
    </h3>
    <h4 class="paper-author-affiliations"><sup>1</sup>INRIA, France</h4>
    <h4 class="paper-author-affiliations"><sup>2</sup>University College London, United Kingdom</h4>
    <h4 class="paper-publisher">10th International Symposium on Visual Computing (ISVC), 2014</h4>
    <br>
    <div>
        <h4>Abstract:</h4>
        <p>
            Surface reconstruction has long been targeted at scan data. With the rise of multi-view acquisition,
            existing surface reconstruction techniques often turn out to be ill adapted to the highly irregular sampling
            and multilayered aspect of such data. In this paper, a novel surface reconstruction technique is
            developed to address these new challenges by means of an advancing front guided by a sphere packing
            methodology. The method is fairly simple and can efficiently triangulate point clouds into high quality
            meshes. The substantiated experimental results demonstrate the robustness and the generality of the proposed
            method.
        </p>
    </div>
    <div>
        <h4>Paper:</h4>
        <a class="btn btn-default" href="/assets/paper/surface_reconstruction_for_multi_view_point_data.pdf"
           role="button" target="_blank">PDF</a>
    </div>
    <div>
        <h4>Images:</h4>
        <div class="row">
            <div class="col-sm-12">
                <a href="/assets/img/research/surface_reconstruction_for_multi_view_point_data/algorithm.png"
                   class="thumbnail">
                    <img src="/assets/img/research/surface_reconstruction_for_multi_view_point_data/algorithm.png"
                         alt="algorithm" style="width:500px;height:418px">
                </a>
                <p>
                    <b>Figure 1:</b> An illustration of the algorithm.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-sm-12">
                <a href="/assets/img/research/surface_reconstruction_for_multi_view_point_data/operations.png"
                   class="thumbnail">
                    <img src="/assets/img/research/surface_reconstruction_for_multi_view_point_data/operations.png"
                         alt="operations" style="width:704px;height:224px">
                </a>
                <p>
                    <b>Figure 2:</b> Three operations applied for advancing the current front (yellow): (a) ear cutting;
                    (b) point addition; (c) merging fronts.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-sm-12">
                <a href="/assets/img/research/surface_reconstruction_for_multi_view_point_data/res_church.png"
                   class="thumbnail">
                    <img src="/assets/img/research/surface_reconstruction_for_multi_view_point_data/res_church.png"
                         alt="res_church" style="width:704px;height:224px">
                </a>
                <p>
                    <b>Figure 3:</b> A point clouds from [25] is triangulated
                    using our proposed method. (a) displays the resulting mesh. (b) is a close-up view.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-sm-12">
                <a href="/assets/img/research/surface_reconstruction_for_multi_view_point_data/pr_vs_ours.png"
                   class="thumbnail">
                    <img src="/assets/img/research/surface_reconstruction_for_multi_view_point_data/pr_vs_ours.png"
                         alt="pr_vs_ours" style="width:501px;height:382px">
                </a>
                <p>
                    <b>Figure 4:</b> A comparison of Poisson reconstruction (a) and our proposed method (b) is
                    illustrated. The input point cloud is same to the one used in in Figure 3a and the two meshes are
                    displayed in the same close-up view. (c) and (d) are two corresponding histograms about triangle
                    angle values.

                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-sm-12">
                <a href="/assets/img/research/surface_reconstruction_for_multi_view_point_data/evaluation.png"
                   class="thumbnail">
                    <img src="/assets/img/research/surface_reconstruction_for_multi_view_point_data/evaluation.png"
                         alt="evaluation" style="width:704px;height:306px">
                </a>
                <p>
                    <b>Figure 5:</b> The reconstruction results of BP, PR and YO in [18,21,2] respectively, as well as
                    our methods are evaluated using the method proposed in [25]. The images show the variance weighted
                    depth difference. Red pixels represent errors larger than 30σ. Green pixels represent the missing
                    scan data of the ground truth. The relative errors between 0 and 30σ are displayed using gray scale
                    from 255 to 0.

                </p>
            </div>
        </div>
    </div>
    <br>
    <br>
</div>